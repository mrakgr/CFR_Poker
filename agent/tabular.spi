inl policy_probs (policy : a st f32) = // 0 <= x for all x in policy should hold.
    inl s = am.reduce (+) policy
    inl a,s = if s = 0 then 1 / f32 (length policy), 0 else 0, 1 / s
    am.map (fun x => a + x * s) policy

inl belief_tabulate slots (action_indices : a u64 st) (at_action_value : a u64 f32) (at_action_weight : a u64 f32) = 
    inl update_head () =
        am.iter4 (fun {policy head={weighted_value weight}} i_action at_action_value at_action_weight =>
            inl add_to l v = inl x = index l i_action in set l i_action (x + v)
            add_to weighted_value (at_action_value * at_action_weight) . add_to weight at_action_weight
            ) slots action_indices at_action_value at_action_weight
    inl action_fun (action_probs : a u64 (a st f32)) (sample_probs : a u64 (a st f32)) =
        inl batch_qs = 
            am.map4 (fun {policy head={weighted_value weight}} i_action sample_prob r =>
                am.mapi2 (fun i_action' weighted_value weight =>
                    inl q = if weight <> 0 then weighted_value / weight else 0
                    if i_action = i_action' then (r - q) / (index sample_prob i_action) + q else q
                    ) weighted_value weight
                ) slots action_indices sample_probs at_action_value
        inl rewards = am.map2 (am.fold2 (fun s a b => s + a * b) 0) batch_qs action_probs
        inl update_policy () =
            am.iter4 (fun {policy={current grad} head} qs mean regret_prob =>
                am.mapInplace (fun i grad => grad + regret_prob * (index qs i - mean)) grad
                ) slots batch_qs rewards at_action_weight
        rewards, update_policy
    update_head, action_fun

type agent_dict present future = dict present {head : dict future {weighted_value : a st f32; weight : a st f32}; policy : {current : a st f32; grad : a st f32}}
inl optimize (agent : agent_dict _ _) =
    dictm.fold (fun () (k,{head policy={current grad}}) =>
        am.mapInplace (fun i x => inl x = max 0 (x + index grad i) in set grad i 0 . x) current
        ) () agent

inl zeros (num_actions : st) : a st f32 = $"numpy.zeros(!num_actions,dtype=numpy.float32)"
inl memo_present agent present num_actions =
    dictm.memoize agent (fun _ => {policy={current=zeros num_actions; grad=zeros num_actions}; head=dictm.empty}) present
inl memo_future head future num_actions =
    dictm.memoize head (fun _ => {weighted_value=zeros num_actions; weight=zeros num_actions}) future
inl memo_slot agent (present,future) num_actions =
    inl {policy head} = memo_present agent present num_actions
    {policy head = memo_future head future num_actions}

inl average (avg, agent : agent_dict _ _ * agent_dict _ _) =
    inl (+.) a b = am.mapInplace (fun (i : st) a => a + index b i) a
    dictm.fold (fun () (present,{head policy={current grad}}) =>
        inl {policy={current=c grad=g} head=h} = memo_present avg present (length current)
        c +. (policy_probs current) . g +. grad
        dictm.fold (fun () (future,{weighted_value weight}) =>
            inl {weighted_value=v weight=w} = memo_future h future (length weighted_value)
            v +. weighted_value . w +. weight
            ) () head
        ) () agent

inl policy forall present future. extractor sel 
        ((agent : agent_dict present future), is_update_head, is_update_policy, epsilon)
        (l : ra u64 _) =
    inl num_batch = length l
    inl create x : a _ _ = create x
    inl at_action_weight = create num_batch
    inl slots = create num_batch
    inl action_probs = create num_batch
    inl sample_probs = create num_batch
    inl action_indices = create num_batch
    inl pids = create num_batch

    inl l : a u64 _ = l |> am.generic.mapi fun i_batch (player_state, game_state, (pid : pid), actions) => 
        inl regret_prob =
            inl prob = pl2_probs player_state pid
            inl env_prob = prob.chance +@ prob.op
            exp (-prob.self.sample + (env_prob.policy - env_prob.sample))
        set at_action_weight i_batch regret_prob

        inl present, future = extractor (player_state, game_state, pid)
        inl actions : a st _ = sel actions
        inl num_actions = length actions
        inl {policy head} = memo_slot agent (present,future) num_actions
        set slots i_batch {policy head}

        inl action_prob = policy_probs policy.current
        set action_probs i_batch action_prob

        inl sample_prob = am.map (fun x => epsilon / f32 num_actions + (1 - epsilon) * x) action_prob
        set sample_probs i_batch sample_prob

        inl i_action = $"numpy.random.choice(!num_actions,p=!sample_prob)"
        set action_indices i_batch i_action

        set pids i_batch pid

        log_prob_from {policy=index action_prob i_action; sample=index sample_prob i_action}, index actions i_action
    l, fun (at_action_reward : a u64 r2) =>
        inl at_action_value = am.map2 (~!!) at_action_reward pids
        inl update_head, action_fun = belief_tabulate slots action_indices at_action_value at_action_weight
        inl corrected_values,update_policy = action_fun action_probs sample_probs
        if is_update_head then update_head()
        if is_update_policy then update_policy()
        am.map2 (fun x pid => if pid = 0 then r2 x else r2 -x) corrected_values pids

// Decays multiplies the head values by the specified scalar factor.
inl head_multiply_ ((agent : agent_dict _ _), w) =
    dictm.fold (fun () (_,{head policy}) =>
        dictm.fold (fun () (_,{weight weighted_value}) =>
            inl f _ x = x * w
            am.mapInplace f weight . am.mapInplace f weighted_value
            ) () head
        ) () agent