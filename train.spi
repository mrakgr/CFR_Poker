inl vs_human game human_pid p =
    let rec loop = function
        | Terminal as g => g
        | Action(player_state,game_state,pid,actions,next) as g => 
            if pid = human_pid then g
            else
                inl cs,_ = p (am.singleton (player_state,game_state,pid,actions))
                loop (next (index cs 0))
    loop game

inl vs_self game (batch_size, p) =
    let rec loop (l : a u64 _) =
        inl terminal_indices : ra u64 _ = am.empty
        inl terminal_rewards : ra u64 r2 = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl data : ra u64 _ = am.empty
        inl nexts : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action (player_state,game_state,pid,actions,next) =>
                rm.add actions_indices i
                rm.add data (player_state,game_state,pid,actions)
                rm.add nexts next
            | Terminal (_,_,x) =>
                rm.add terminal_indices i
                rm.add terminal_rewards x
        inl actions_rewards : a _ _ =
            if 0 < length data then
                inl (cs : a _ _),update = p data
                am.generic.map2 (<|) nexts cs |> loop |> update
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices actions_rewards
        am.iter2 (set rewards_all) terminal_indices terminal_rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)

inl vs_one game (batch_size, p1, p2) =
    let rec loop (l : a u64 _) =
        inl terminal_indices : ra u64 _ = am.empty
        inl terminal_rewards : ra u64 r2 = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl p1_data : ra u64 _ = am.empty
        inl p2_data : ra u64 _ = am.empty
        inl p1_nexts : ra u64 _ = am.empty
        inl p2_nexts : ra u64 _ = am.empty
        inl pids : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action (player_state,game_state,pid,actions,next) =>
                rm.add actions_indices i
                if pid = 0 then
                    rm.add p1_data (player_state,game_state,pid,actions)
                    rm.add p1_nexts next
                else
                    rm.add p2_data (player_state,game_state,pid,actions)
                    rm.add p2_nexts next
                rm.add pids pid
            | Terminal (_,_,x) =>
                rm.add terminal_indices i
                rm.add terminal_rewards x
        inl actions_rewards =
            if 0 < length pids then
                inl p1_cs,p1_update : a _ _ * (a u64 r2 -> a u64 r2) = p1 p1_data
                inl p2_cs,p2_update : a _ _ * (a u64 r2 -> a u64 r2) = p2 p2_data
                inl p1_results = array.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl len = length p1_cs
                inl slice_from forall d t. (x : a d t) (i : d) : a d t = $"!x[!i:]"
                inl slice_near_to forall d t. (x : a d t) (i : d) : a d t = $"!x[:!i]"
                inl p1_rs = p1_update (slice_near_to rs len)
                inl p2_rs = p2_update (slice_from rs len)
                am.mapFold (fun (p1_i,p2_i) i => if i = 0 then index p1_rs p1_i,p1_i+1,p2_i else index p2_rs p2_i,p1_i,p2_i+1)
                    (0,0) pids |> fst
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices actions_rewards
        am.iter2 (set rewards_all) terminal_indices terminal_rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)

inl neural_handler forall (actions : * -> *) act. 
        extractor (schema : schema _ _ _) (max_actions, action_selector : st * (actions act -> a st act)) // Applied on the Spiral side.
        policy_value_action_eval // Applied on the Python side.
        (data : ra _ (_ * _ * pid * actions act)) = // Applied on the Spiral side.
    if length data = 0 then am.empty, id else
    !!!!Global("import torch")
    inl policy_size = serialization.dense.array.size schema.policy
    inl value_size = serialization.dense.array.size schema.value
    inl action_size = serialization.dense.array.size schema.action
    inl batch_size = length data
    inl policy_seq_size, value_seq_size = 
        am.fold (fun (s1,s2) (player_state,game_state,pid,actions) => 
            inl a,b = extractor (player_state,game_state,pid)
            max s1 (listm.length a), max s2 (listm.length b)
            ) (0u64, 0u64) data
    inl policy_data : a3 f32 = $"numpy.zeros((!batch_size,!policy_seq_size,!policy_size),dtype=numpy.float32)"
    inl policy_mask : a2 i8 = $"numpy.zeros((!batch_size,!policy_seq_size),dtype=numpy.int8)"
    inl value_data : a3 f32 = $"numpy.zeros((!batch_size,!value_seq_size,!value_size),dtype=numpy.float32)"
    inl value_mask : a2 i8 = $"numpy.zeros((!batch_size,!value_seq_size),dtype=numpy.int8)"
    inl action_data : a3 f32 = $"numpy.zeros((!batch_size,!max_actions,!action_size),dtype=numpy.float32)"
    inl action_mask : a2 i8 = $"numpy.ones((!batch_size,!max_actions),dtype=numpy.int8)"
    inl action_selected : a u64 (a st act) = create batch_size
    inl pids : a u64 f32 = create batch_size
    data |> am.iteri fun i_batch (player_state,game_state,pid,actions) =>
        inl policy, value = extractor (player_state,game_state,pid)
        inl f forall y. (x : a3 y) (i_seq : u64) : a st y = $"!x[!i_batch,!i_seq]"

        inl i_seq = listm.fold (fun i_seq x => schema.policy.pickle x (0, f policy_data i_seq) . i_seq + 1) 0 policy
        loop.for' {from=i_seq; nearTo=policy_seq_size} fun i_seq => $"!policy_mask[!i_batch,!i_seq] = 1"

        inl i_seq = listm.fold (fun i_seq x => schema.value.pickle x (0, f value_data i_seq) . i_seq + 1) 0 value
        loop.for' {from=i_seq; nearTo=value_seq_size} fun i_seq => $"!value_mask[!i_batch,!i_seq] = 1"
        
        inl acts = action_selector actions
        set action_selected i_batch acts
        acts |> am.iteri (fun i_act act =>
            schema.action.pickle act (0, $"!action_data[!i_batch,!i_act]")
            $"!action_mask[!i_batch,!i_act] = 0"
            )
        set pids i_batch (if pid = 0 then 1 else -1)

    inl from_numpy x = $"torch.from_numpy(!x)" : tensor
    inl from_numpy_as_bool x = from_numpy ($"!x.view('bool')" : obj)
    inl action_probs, sample_indices, update : a2 f32 * a u64 i64 * (obj -> obj) = 
        inl x : obj = 
            policy_value_action_eval (
                from_numpy policy_data, from_numpy_as_bool policy_mask, 
                from_numpy value_data, from_numpy_as_bool value_mask, 
                from_numpy pids, 
                from_numpy action_data, from_numpy_as_bool action_mask
                )
        $"!x[0]", $"!x[1]", $"!x[2]"
    inl cs : a u64 _ = am.init batch_size fun b =>
        inl a = index sample_indices b
        log_prob_from_sample (if $"!action_probs is None" then 1.0 else $"!action_probs[!b,!a]"), index (index action_selected b) (conv_int a)
    
    cs, update

inl neural extractor schema sel =
    inl handler = neural_handler extractor schema sel
    inl size =
        inl policy = serialization.dense.array.size schema.policy
        inl value = serialization.dense.array.size schema.value
        inl action = serialization.dense.array.size schema.action
        namedtuple "Size" {action policy value}
    namedtuple "Neural" {handler size}